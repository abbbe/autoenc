{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thursday 4/03/2021\n",
    "\n",
    "* Visualize YYs (third output sheet or animated GIF) to assess decoder performance\n",
    "* Rerun with [2, 10, 50] epochs\n",
    "* Implement beta-VAE (see beta-vae notebook)\n",
    "\n",
    "Next:\n",
    "\n",
    "* Explore impact of changing filters chain down to 2x (2,2)\n",
    "* Scatterplot 3D\n",
    "* How does it fluctuate depending on network architecture, nlats, training protocol, etc\n",
    "* Explore fold area in L0/L1 space. For each frame show:\n",
    " * image of gym robot,\n",
    " * colored L0/L1 scatterplot, with a red cross showing current latent state,\n",
    " * image reconstructed by the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OoMQk_5c4UYm",
    "outputId": "02cc896f-38e0-4a92-81ce-c20a7322369d"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of dataset load helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_dataset(filename):\n",
    "    with open(filename, 'rb') as handle:\n",
    "        dataset = pickle.load(handle)\n",
    "        if isinstance(dataset, dict):\n",
    "            return dataset\n",
    "        return dataset[0] # dataset-random-100k.mdict.pickle case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = load_dataset('dataset-random-100k.mdict.pickle')\n",
    "#dataset = load_dataset('dataset-grid-10-1000.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of env image vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_ax(ax):\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "def _imshow(ax, img):\n",
    "    if img is not None:\n",
    "        ax.imshow(img, cmap='Greys', origin='lower')\n",
    "    _clean_ax(ax)\n",
    "        \n",
    "def plot_Y(img_array):\n",
    "    assert(img_array.shape[-1] == 1)\n",
    "    fig, axs = plt.subplots(figsize=(2, 2))\n",
    "    _imshow(axs, img_array[..., 0])\n",
    "    return fig, axs\n",
    "\n",
    "def plot_Ys(img_array, title=None, ncols=5):\n",
    "    assert(ncols > 0)\n",
    "\n",
    "    nimgs = img_array.shape[0]\n",
    "    if nimgs == 0:\n",
    "        return None, None\n",
    "    elif nimgs == 1:\n",
    "        return plot_Y(img_array[0,...])\n",
    "    elif nimgs <= ncols:\n",
    "        ncols = nimgs\n",
    "        fig, axs = plt.subplots(1, ncols, figsize=(2*ncols, 2))\n",
    "\n",
    "        for i in range(ncols):\n",
    "            _imshow(axs[i], img_array[i,...])\n",
    "    else:\n",
    "        nrows = int((nimgs-1)/ ncols) + 1\n",
    "        fig, axs = plt.subplots(nrows, ncols, figsize=(2*ncols, 2*nrows))\n",
    "\n",
    "        for i in range(nrows):\n",
    "            for j in range(ncols):\n",
    "                k = i*ncols + j\n",
    "                if k < nimgs:\n",
    "                    img = img_array[k,...]\n",
    "                else:\n",
    "                    img = None\n",
    "                _imshow(axs[i][j], img)\n",
    "                    \n",
    "    if title is not None:\n",
    "        fig.suptitle(\"%s (%s)\" % (title, str(img_array.shape)))\n",
    "        \n",
    "    return fig, axs\n",
    "\n",
    "def plot_Y_YY(img_array1, img_array2, title=None, fig=None, axs=None):\n",
    "    if fig is None and axs is None:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(4, 2))\n",
    "        \n",
    "    _imshow(axs[0], img_array1)\n",
    "    _imshow(axs[1], img_array2)\n",
    "\n",
    "    if title is not None:\n",
    "        fig.suptitle(title)\n",
    "        \n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _=plot_Ys(dataset['Y'][0:10,...], title=\"First 10 elements of dataset['Y']\", ncols=5)\n",
    "# _=plot_Y_YY(dataset_grid['Y'][0], dataset_grid_out['YY'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of data (in and lat) visualization helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_scatter(data1, x1, i1, data2, x2, i2):\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    ax.scatter(data1[x1][:,i1], data2[x2][:,i2], 1)\n",
    "    ax.set_xlabel(\"%s%d\" % (x1, i1))\n",
    "    ax.set_ylabel(\"%s%d\" % (x2, i2))\n",
    "    ax.grid()\n",
    "\n",
    "def fine_scatter_color(data1, x1, i1, data2, x2, i2, c=None, size=10, ax=None):\n",
    "    if ax is None:\n",
    "        _fig, ax = plt.subplots(figsize=(size, size))\n",
    "        \n",
    "    ax.set_facecolor('xkcd:black')\n",
    "\n",
    "    x, y = data1[x1][:,i1], data2[x2][:,i2]\n",
    "    ax.scatter(x, y, 1, c=c)\n",
    "    ax.set_xlabel(\"%s%d\" % (x1, i1))\n",
    "    ax.set_ylabel(\"%s%d\" % (x2, i2))\n",
    "    #ax.grid()\n",
    "\n",
    "def fine_scatter_sum(data1, x1, i1a, i1b, data2, x2, i2):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    x = data1[x1][:,i1a] + data1[x1][:,i1b]\n",
    "    ax.scatter(x, data2[x2][:,i2], 1)\n",
    "    \n",
    "    ax.set_xlabel(\"%s(%d+%d)\" % (x1, i1a, i1b))\n",
    "    ax.set_ylabel(\"%s%d\" % (x2, i2))\n",
    "    ax.grid()\n",
    "\n",
    "def fine_scatter_color_sum(data1, x1, i1a, i1b, data2, x2, i2, c=None, size=10, ax=None):\n",
    "    if ax is None:\n",
    "        _fig, ax = plt.subplots(figsize=(size, size))\n",
    "        \n",
    "    ax.set_facecolor('xkcd:black')\n",
    "\n",
    "    x = data1[x1][:,i1a] + data1[x1][:,i1b]\n",
    "    y = data2[x2][:,i2]\n",
    "    ax.scatter(x, data2[x2][:,i2], 1, c)\n",
    "    \n",
    "    ax.set_xlabel(\"%s(%d+%d)\" % (x1, i1a, i1b))\n",
    "    ax.set_ylabel(\"%s%d\" % (x2, i2))\n",
    "\n",
    "def fine_scatter_color_sub(data1, x1, i1a, i1b, data2, x2, i2, c=None, size=10, ax=None):\n",
    "    if ax is None:\n",
    "        _fig, ax = plt.subplots(figsize=(size, size))\n",
    "        \n",
    "    ax.set_facecolor('xkcd:black')\n",
    "\n",
    "    x = data1[x1][:,i1a] - data1[x1][:,i1b]\n",
    "    y = data2[x2][:,i2]\n",
    "    ax.scatter(x, data2[x2][:,i2], 1, c)\n",
    "    \n",
    "    ax.set_xlabel(\"%s(%d-%d)\" % (x1, i1a, i1b))\n",
    "    ax.set_ylabel(\"%s%d\" % (x2, i2))\n",
    "\n",
    "def display_xvars(data):\n",
    "    nLat = data['L'].shape[1]\n",
    "    nVoltages = data['A'].shape[1]\n",
    "\n",
    "    fig, axs = plt.subplots(nLat+1, nVoltages+1)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for i in range(nLat):\n",
    "      for j in range(nVoltages):\n",
    "        title = \"volt%d vs lat%d\" % (j, i)\n",
    "        axs[i][j].title.set_text(title)\n",
    "        axs[i][j].plot(data['L'][:, i], data['A'][:,j], '.')\n",
    "\n",
    "    axs[nLat][0].title.set_text(\"volt0 vs volt1\")\n",
    "    axs[nLat][0].plot(data['A'][:,1], data['A'][:,0], '.')\n",
    "\n",
    "    axs[0][nVoltages].title.set_text(\"lat1 vs lat0\")\n",
    "    axs[0][nVoltages].plot(data['L'][:,0], data['L'][:,1], '.')\n",
    "\n",
    "    _clean_ax(axs[1][2])\n",
    "    _clean_ax(axs[2][1])\n",
    "    _clean_ax(axs[2][2])\n",
    "\n",
    "def cycle_autoencoder0(ae, data, prefix=\"\", N=10):\n",
    "    out_data = dict()\n",
    "    \n",
    "    Y = data['Y']\n",
    "    \n",
    "    print(\"Y.shape=\" + str(Y.shape))\n",
    "    YY = ae['ae'].predict(Y)\n",
    "    YY = np.array(YY)\n",
    "    print(\"YY.shape=\" + str(YY.shape))\n",
    "    \n",
    "    out_data['YY'] = YY\n",
    "\n",
    "    # - Y\n",
    "    plot_Ys(Y[0:N,...], title=\"First %d elements of %sY\" % (N, prefix))\n",
    "   \n",
    "    # - YY\n",
    "    plot_Ys(YY[0:N,...], title=\"First %d elements of %sYY\" % (N, prefix))\n",
    "    \n",
    "    return out_data\n",
    "\n",
    "def cycle_autoencoder(ae, data, prefix=\"\", N=10, vae=False, display=False):\n",
    "    Y = data['Y']\n",
    "    \n",
    "    if display:\n",
    "        print(\"Y.shape=\" + str(Y.shape))\n",
    "    L = ae['enc'].predict(Y)\n",
    "    L = np.array(L)\n",
    "    if display:\n",
    "        print(\"L.shape=\" + str(L.shape))\n",
    "\n",
    "    if vae:\n",
    "        L = np.array(L)[2] # [z_mean, z_log_var, z] - take Z\n",
    "\n",
    "    YY = ae['dec'].predict(L)\n",
    "    YY = np.array(YY)\n",
    "    if display:\n",
    "        print(\"YY.shape=\" + str(YY.shape))\n",
    "    \n",
    "    data['L'] = L # FIXME\n",
    "    data['YY'] = YY # FIXME\n",
    "\n",
    "    out_data = dict()\n",
    "    out_data['L'] = L\n",
    "    out_data['YY'] = YY\n",
    "\n",
    "    # - Y\n",
    "    if display:\n",
    "        plot_Ys(Y[0:N,...], title=\"First %d elements of %sY\" % (N, prefix))\n",
    "\n",
    "        # - L\n",
    "        fig, axs = plt.subplots(1, L.shape[1])\n",
    "        fig.suptitle(\"Latent variables %s\" % (str(L.shape)))\n",
    "        for i in range(L.shape[1]):\n",
    "            axs[i].hist(L[:,i])    \n",
    "\n",
    "        # - YY\n",
    "        plot_Ys(YY[0:N,...], title=\"First %d elements of %sYY\" % (N, prefix))\n",
    "\n",
    "        display_xvars(data)\n",
    "    \n",
    "    return out_data    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementations of model save and load methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(model, prefix):\n",
    "    model['ae'].save(prefix + '-ae.h5')\n",
    "    model['enc'].save(prefix + '-enc.h5')\n",
    "    model['dec'].save(prefix + '-dec.h5')\n",
    "    print(\"Models saved to \" + prefix + \" ...\")\n",
    "\n",
    "def load_models(prefix):\n",
    "    model = dict()\n",
    "    for k in ['ae', 'enc', 'dec']:\n",
    "        fname = prefix + '-' + k + '.h5'\n",
    "        model[k] = keras.models.load_model(fname)\n",
    "    return model\n",
    "\n",
    "# VAE models cannot be saved by the above methods, can only save weights\n",
    "\n",
    "def save_models_weights(model, prefix):\n",
    "    model['ae'].save_weights(prefix + '-ae.h5w')\n",
    "    model['enc'].save_weights(prefix + '-enc.h5w')\n",
    "    model['dec'].save_weights(prefix + '-dec.h5w')\n",
    "    print(\"Models weights saved to \" + prefix + \" ...\")\n",
    "    \n",
    "def load_models_weights(model, prefix):\n",
    "    model['ae'].load_weights(prefix + '-ae.h5w')\n",
    "    model['enc'].load_weights(prefix + '-enc.h5w')\n",
    "    model['dec'].load_weights(prefix + '-dec.h5w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize L0/L1 space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_lat_space(dataset_grid, dataset_grid_out, sheet):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15,12))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if sheet == 1:\n",
    "        fine_scatter_color(dataset_grid, 'A', 0, dataset_grid_out, 'L' , 0, dataset_grid['A'][:,1], ax=axs[0,0])\n",
    "        fine_scatter_color(dataset_grid, 'A', 1, dataset_grid_out, 'L' , 1, dataset_grid['A'][:,0], ax=axs[0,1])\n",
    "\n",
    "        fine_scatter_color(dataset_grid, 'A', 0, dataset_grid_out, 'L' , 1, dataset_grid['A'][:,1], ax=axs[1,0])\n",
    "        fine_scatter_color(dataset_grid, 'A', 1, dataset_grid_out, 'L' , 0, dataset_grid['A'][:,0], ax=axs[1,1])\n",
    "\n",
    "    elif sheet == 2:\n",
    "        fine_scatter_color_sum(dataset_grid, 'A', 0, 1, dataset_grid_out, 'L', 0, dataset_grid_out['L'][:,1], ax=axs[0,0])\n",
    "        fine_scatter_color_sum(dataset_grid, 'A', 0, 1, dataset_grid_out, 'L', 1, dataset_grid_out['L'][:,0], ax=axs[0,1])\n",
    "\n",
    "        fine_scatter_color_sub(dataset_grid, 'A', 0, 1, dataset_grid_out, 'L', 0, dataset_grid_out['L'][:,1], ax=axs[1,0])\n",
    "        fine_scatter_color_sub(dataset_grid, 'A', 0, 1, dataset_grid_out, 'L', 1, dataset_grid_out['L'][:,0], ax=axs[1,1])\n",
    "    else:\n",
    "        raise\n",
    "    \n",
    "    return fig, axs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vae import build_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_params = {'epochs': 2, 'convs': [16, 8, 8]}\n",
    "#autoencoder = build_autoencoder((64, 64, 1), 2, build_params)\n",
    "\n",
    "#load_models_weights(autoencoder, \"trained-models/20210302-vae\") # NB: build the model first\n",
    "# or\n",
    "#train_model(autoencoder, dataset, val_dataset, build_params, \"test1037\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vae import build_autoencoder\n",
    "\n",
    "DATASETDIR = \"datasets.arm\"\n",
    "\n",
    "dataset = load_dataset(DATASETDIR + \"/dataset-random-100k.pickle\")\n",
    "dataset_grid = load_dataset(DATASETDIR + \"/dataset-grid-10-1000.pickle\")\n",
    "#dataset_val = load_dataset(DATASETDIR + \"/dataset-random-10k.pickle\")\n",
    "\n",
    "def shuffle_and_cut(ds, N):\n",
    "    np.random.shuffle(ds['A'])\n",
    "    np.random.shuffle(ds['Y'])\n",
    "    \n",
    "    ds['A'] = ds['A'][:N]\n",
    "    ds['Y'] = ds['Y'][:N]\n",
    "    \n",
    "#shuffle_and_cut(dataset, 25000)\n",
    "#shuffle_and_cut(dataset_grid, 5000)\n",
    "#shuffle_and_cut(dataset_val, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "def animate_Y_YYs(ys, yys, outfile):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(4, 2))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    _clean_ax(axs[0])\n",
    "    _clean_ax(axs[1])\n",
    "\n",
    "    def animate(frame):\n",
    "        _imshow(axs[0], ys[frame])\n",
    "        _imshow(axs[1], yys[frame])\n",
    "        return (fig,)\n",
    "\n",
    "    N = ys.shape[0]\n",
    "    assert(N == yys.shape[0])\n",
    "    ani = animation.FuncAnimation(fig, animate, frames=range(N), blit=True)\n",
    "    ani.save(outfile)\n",
    "    fig.clf()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, datetime\n",
    "\n",
    "BASE_DIR = \"20210304-vae2.arm/rand_100k-grid_10_1000\"\n",
    "TENSORBOARD_LOGS_DIR =  \"%s/tensorboard-logs\" % BASE_DIR\n",
    "TRAINED_MODELS_DIR = \"%s/trained-models\" % BASE_DIR\n",
    "IMGS_DIR = \"%s/imgs\" % BASE_DIR\n",
    "\n",
    "for dir in [BASE_DIR, TENSORBOARD_LOGS_DIR, TRAINED_MODELS_DIR, IMGS_DIR]:\n",
    "    if not os.path.isdir(dir):\n",
    "        os.mkdir(dir)\n",
    "\n",
    "def train_model(model, dataset, val_dataset, build_params, suffix):\n",
    "    ae = model['ae']\n",
    "    \n",
    "    logdir = os.path.join((\"%s/%s\" % (TENSORBOARD_LOGS_DIR, suffix)), datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir) #, histogram_freq=1)\n",
    "    \n",
    "    # FIXME ae.fit(dataset['Y'], validation_data=(val_dataset['Y'], val_dataset['Y']), callbacks=[tensorboard_callback],\n",
    "    ae.fit(dataset['Y'], callbacks=[tensorboard_callback],\n",
    "        epochs=build_params['epochs'], batch_size=128, verbose=0)\n",
    "    \n",
    "    save_models_weights(model, \"%s/%s\" % (TRAINED_MODELS_DIR, suffix))\n",
    "    \n",
    "def cycle(dataset, dataset_grid, dataset_val, build_params, suffix):\n",
    "    print(\"*** run(%s, %s)\" % (str(build_params), suffix))\n",
    "    \n",
    "    vis_fname = \"%s/%sa.png\" % (IMGS_DIR, suffix)\n",
    "    vis_fname2 = \"%s/%sb.png\" % (IMGS_DIR, suffix)\n",
    "    ani_fname = \"%s/%s.mp4\" % (IMGS_DIR, suffix)\n",
    "    \n",
    "    if os.path.isfile(vis_fname):\n",
    "        print(\"%s exists, skipping ...\" % vis_fname)\n",
    "        return\n",
    "    \n",
    "    print(\"building and training vae, build_params=%s\" % str(build_params))\n",
    "    keras.backend.clear_session()\n",
    "    autoencoder = build_autoencoder((64, 64, 1), 2, build_params)\n",
    "    train_model(autoencoder, dataset, dataset_val, build_params, suffix)\n",
    "\n",
    "    print(\"running vae on grid dataset, saving visualization into %s\" % (vis_fname))    \n",
    "    dataset_grid_out = cycle_autoencoder(autoencoder, dataset_grid, vae=True, display=False)\n",
    "    \n",
    "    fig, axs = visualize_lat_space(dataset_grid, dataset_grid_out, sheet=1)\n",
    "    fig.savefig(vis_fname)\n",
    "    fig.clf()\n",
    "    \n",
    "    fig, axs = visualize_lat_space(dataset_grid, dataset_grid_out, sheet=2)\n",
    "    fig.savefig(vis_fname2)\n",
    "    fig.clf()\n",
    "\n",
    "    NANIFRAMES = 20\n",
    "    print(\"saving ani visualization (for %d datapoints from grid dataset) into %s\" % (NANIFRAMES, ani_fname))\n",
    "    rng = np.random.default_rng()\n",
    "    ani_i = rng.choice(range(dataset_grid['Y'].shape[0]), size=NANIFRAMES) # choose 20 random items from the dataset_grid\n",
    "    animate_Y_YYs(dataset_grid['Y'][ani_i], dataset_grid_out['YY'][ani_i], outfile=ani_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build_params = {'epochs': 2, 'convs': [16, 8, 8]}\n",
    "#autoencoder = build_autoencoder((64, 64, 1), 2, build_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_models_weights(autoencoder, \"trained-models/20210302-vae\") # NB: build the model first\n",
    "#train_model(autoencoder, dataset, dataset_val, build_params, \"test1037\")\n",
    "#dataset_val_out = cycle_autoencoder(autoencoder, dataset_val, vae=True)\n",
    "#animate_Y_YYs(dataset_val['Y'][:20], dataset_val_out['YY'][:20], outfile='test-animate_Y_YYs.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cycle(dataset, dataset_grid, None, {'epochs': 1, 'convs': [16, 8, 8]}, \"test-cycle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** run({'epochs': 5, 'convs': [16, 8, 8]}, epochs_5-0)\n",
      "building and training vae, build_params={'epochs': 5, 'convs': [16, 8, 8]}\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 16)   160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 8)    1160        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 8)    584         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 32768)        0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            65538       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            65538       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sampling (Sampling)             (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 132,980\n",
      "Trainable params: 132,980\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32768)             98304     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 64, 64, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 64, 64, 8)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 64, 64, 8)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 1)         17        \n",
      "=================================================================\n",
      "Total params: 100,657\n",
      "Trainable params: 100,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Models weights saved to 20210304-vae2.arm/rand_100k-grid_10_1000/trained-models/epochs_5-0 ...\n",
      "running vae on grid dataset, saving visualization into 20210304-vae2.arm/rand_100k-grid_10_1000/imgs/epochs_5-0a.png\n",
      "saving ani visualization (for 20 datapoints from grid dataset) into 20210304-vae2.arm/rand_100k-grid_10_1000/imgs/epochs_5-0.mp4\n",
      "*** run({'epochs': 5, 'convs': [16, 8, 8]}, epochs_5-1)\n",
      "building and training vae, build_params={'epochs': 5, 'convs': [16, 8, 8]}\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 16)   160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 8)    1160        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 8)    584         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 32768)        0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            65538       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            65538       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sampling (Sampling)             (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 132,980\n",
      "Trainable params: 132,980\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32768)             98304     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 64, 64, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 64, 64, 8)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 64, 64, 8)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 1)         17        \n",
      "=================================================================\n",
      "Total params: 100,657\n",
      "Trainable params: 100,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Models weights saved to 20210304-vae2.arm/rand_100k-grid_10_1000/trained-models/epochs_5-1 ...\n",
      "running vae on grid dataset, saving visualization into 20210304-vae2.arm/rand_100k-grid_10_1000/imgs/epochs_5-1a.png\n",
      "saving ani visualization (for 20 datapoints from grid dataset) into 20210304-vae2.arm/rand_100k-grid_10_1000/imgs/epochs_5-1.mp4\n",
      "*** run({'epochs': 5, 'convs': [16, 8, 8]}, epochs_5-2)\n",
      "building and training vae, build_params={'epochs': 5, 'convs': [16, 8, 8]}\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 16)   160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 8)    1160        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 8)    584         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 32768)        0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            65538       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            65538       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sampling (Sampling)             (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 132,980\n",
      "Trainable params: 132,980\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32768)             98304     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 64, 64, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 64, 64, 8)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 64, 64, 8)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 1)         17        \n",
      "=================================================================\n",
      "Total params: 100,657\n",
      "Trainable params: 100,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models weights saved to 20210304-vae2.arm/rand_100k-grid_10_1000/trained-models/epochs_5-2 ...\n",
      "running vae on grid dataset, saving visualization into 20210304-vae2.arm/rand_100k-grid_10_1000/imgs/epochs_5-2a.png\n",
      "saving ani visualization (for 20 datapoints from grid dataset) into 20210304-vae2.arm/rand_100k-grid_10_1000/imgs/epochs_5-2.mp4\n",
      "*** run({'epochs': 5, 'convs': [16, 8, 8]}, epochs_5-3)\n",
      "building and training vae, build_params={'epochs': 5, 'convs': [16, 8, 8]}\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 16)   160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 8)    1160        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 8)    584         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 32768)        0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            65538       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            65538       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sampling (Sampling)             (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 132,980\n",
      "Trainable params: 132,980\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32768)             98304     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 64, 64, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 64, 64, 8)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 64, 64, 8)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 1)         17        \n",
      "=================================================================\n",
      "Total params: 100,657\n",
      "Trainable params: 100,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Models weights saved to 20210304-vae2.arm/rand_100k-grid_10_1000/trained-models/epochs_5-3 ...\n",
      "running vae on grid dataset, saving visualization into 20210304-vae2.arm/rand_100k-grid_10_1000/imgs/epochs_5-3a.png\n",
      "saving ani visualization (for 20 datapoints from grid dataset) into 20210304-vae2.arm/rand_100k-grid_10_1000/imgs/epochs_5-3.mp4\n",
      "*** run({'epochs': 5, 'convs': [16, 8, 8]}, epochs_5-4)\n",
      "building and training vae, build_params={'epochs': 5, 'convs': [16, 8, 8]}\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 16)   160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 8)    1160        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 8)    584         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 32768)        0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            65538       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            65538       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sampling (Sampling)             (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 132,980\n",
      "Trainable params: 132,980\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32768)             98304     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 64, 64, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 64, 64, 8)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 64, 64, 8)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 1)         17        \n",
      "=================================================================\n",
      "Total params: 100,657\n",
      "Trainable params: 100,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Models weights saved to 20210304-vae2.arm/rand_100k-grid_10_1000/trained-models/epochs_5-4 ...\n",
      "running vae on grid dataset, saving visualization into 20210304-vae2.arm/rand_100k-grid_10_1000/imgs/epochs_5-4a.png\n",
      "saving ani visualization (for 20 datapoints from grid dataset) into 20210304-vae2.arm/rand_100k-grid_10_1000/imgs/epochs_5-4.mp4\n",
      "*** run({'epochs': 10, 'convs': [16, 8, 8]}, epochs_10-0)\n",
      "building and training vae, build_params={'epochs': 10, 'convs': [16, 8, 8]}\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 16)   160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 8)    1160        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 8)    584         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 32768)        0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            65538       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            65538       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sampling (Sampling)             (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 132,980\n",
      "Trainable params: 132,980\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32768)             98304     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 64, 64, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 64, 64, 8)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 64, 64, 8)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 1)         17        \n",
      "=================================================================\n",
      "Total params: 100,657\n",
      "Trainable params: 100,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models weights saved to 20210304-vae2.arm/rand_100k-grid_10_1000/trained-models/epochs_10-0 ...\n",
      "running vae on grid dataset, saving visualization into 20210304-vae2.arm/rand_100k-grid_10_1000/imgs/epochs_10-0a.png\n",
      "saving ani visualization (for 20 datapoints from grid dataset) into 20210304-vae2.arm/rand_100k-grid_10_1000/imgs/epochs_10-0.mp4\n",
      "*** run({'epochs': 10, 'convs': [16, 8, 8]}, epochs_10-1)\n",
      "building and training vae, build_params={'epochs': 10, 'convs': [16, 8, 8]}\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 16)   160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 8)    1160        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 8)    584         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 32768)        0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            65538       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            65538       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sampling (Sampling)             (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 132,980\n",
      "Trainable params: 132,980\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32768)             98304     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 64, 64, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 64, 64, 8)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 64, 64, 8)         584       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 16)        1168      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 1)         17        \n",
      "=================================================================\n",
      "Total params: 100,657\n",
      "Trainable params: 100,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for epochs in [5, 10]:\n",
    "    for i in range(5):\n",
    "        suffix = \"epochs_%d-%d\" % (epochs, i)\n",
    "        cycle(dataset, dataset_grid, None, {'epochs': epochs, 'convs': [16, 8, 8]}, suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for epochs in [100]:\n",
    "    for i in range(5):\n",
    "        suffix = \"convs_16_8_8-epochs_%d-%d\" % (epochs, i)\n",
    "        cycle(dataset, val_dataset, {'epochs': epochs, 'convs': [16, 8, 8]}, suffix)\n",
    "        \n",
    "for epochs in [5, 25, 50, 100]:\n",
    "    for i in range(5):\n",
    "        suffix = \"convs_16_8_4-epochs_%d-%d\" % (epochs, i)\n",
    "        cycle(dataset, val_dataset, {'epochs': epochs, 'convs': [16, 8, 4]}, suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "robot-arm-vae2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
